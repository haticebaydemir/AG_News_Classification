{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOkmv/22+4SE/Arw5mSiOD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haticebaydemir/AG_News_Classification/blob/main/AG_News_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proje: AG News Sınıflandırma\n",
        "\n",
        "Hedef: AG News veri seti kullanarak haber metinlerini 4 ana kategoriye (World, Sports, Business, Science/Technology) sınıflandırmak."
      ],
      "metadata": {
        "id": "-pchht25KJ-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kütüphaneler"
      ],
      "metadata": {
        "id": "Nxk_DV7u6tN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kullanılan Araçlar ve Kütüphaneler:\n",
        "\n",
        "Hugging Face Transformers: Önceden eğitilmiş dil modellerini kullanmak için.\n",
        "\n",
        "PyTorch: Model eğitimi ve optimizasyonu için.\n",
        "\n",
        "Datasets: Veri setlerini kolayca yönetmek için."
      ],
      "metadata": {
        "id": "JfbnPO4UKO1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch datasets\n",
        "# Hugging Face’in transformers kütüphanesi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qWaNNkh0Q49",
        "outputId": "ffe4b085-a13e-4e3c-eadb-ffbc3174351d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "oxH7F4Kq0vbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DistilBertTokenizer: DistilBERT modeline uygun şekilde metinleri tokenize etmek için kullanılır.\n",
        "\n",
        "DistilBertForSequenceClassification: Metin sınıflandırması yapmak için DistilBERT modelini kullanır."
      ],
      "metadata": {
        "id": "fMvVm-RPQbEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Veri Setini Yükleme ve İnceleme"
      ],
      "metadata": {
        "id": "hMGwM0Y66zJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 'AG News' veri setini kullanacağız, çünkü bu veri seti haber başlıklarını dört farklı kategoriye ayırır: iş, spor, bilim ve dünya."
      ],
      "metadata": {
        "id": "Pfp8BAxn6kYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AG News veri setini yükle\n",
        "dataset = load_dataset(\"ag_news\", split='train[:1000]')  # İlk 1000 örneği al\n",
        "\n",
        "# Veri setinin örneklerini inceleyin\n",
        "for example in dataset:\n",
        "    print(f\"Label: {example['label']}, Text: {example['text']}\")\n",
        "    break  # İlk örneği görüntüle, fazla veri dökümünü önler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rieYDeUa0xzu",
        "outputId": "0efa3ee5-ac67-43d6-f683-f9d48229a9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 2, Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Veriyi Tokenizasyon"
      ],
      "metadata": {
        "id": "mYmn7EF663AU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizasyon: Modelin anlayabileceği sayısal veriler üretmek için metinler tokenize edilir. Bu işlem, metinleri kelime parçacıkları veya token'lara böler.\n",
        "\n",
        "Bu adımda, metinleri DistilBERT modeline uygun hale getireceğiz.\n"
      ],
      "metadata": {
        "id": "d3pPI3x665-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Yükleme\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') #Bu satır, DistilBERT modelinin önceden eğitilmiş tokenizer'ını yükler. Tokenizer, metinleri modelin anlayacağı forma dönüştürür.\n",
        "\n",
        "# Tokenizasyon işlemi\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "# Tokenizasyon\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Sütunları yeniden adlandırma ve formatlama\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# Eğitim ve test veri setlerine bölme\n",
        "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n"
      ],
      "metadata": {
        "id": "jpKHUtj100cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Seçimi ve Eğitim"
      ],
      "metadata": {
        "id": "YubQ0wvX7B8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeli Eğitme: Eğitim verileri üzerinde model eğitilir. Eğitim sırasında model, her haber metnini doğru sınıfa atamayı öğrenir."
      ],
      "metadata": {
        "id": "DM9H4KlfFyzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeli Yükleme\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)\n",
        "\n",
        "# Eğitim argümanları\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Trainer Nesnesini Oluşturma\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Modeli Eğitme\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1tKeKCSR02dF",
        "outputId": "4712d051-9f83-4b0a-da9d-30c044b0e9b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 30:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.370800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.375600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.346000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.337400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.323300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.265400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.196200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.209600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.111700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.075900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.969700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.713800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.856600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.692900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.664700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.522500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.728600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.628400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.519900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.506000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.368000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.321000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.320200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.406100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.176100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.415900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.507700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.156600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.340100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.699300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=300, training_loss=0.7708656521638234, metrics={'train_runtime': 1858.0543, 'train_samples_per_second': 1.292, 'train_steps_per_second': 0.161, 'total_flos': 79483274035200.0, 'train_loss': 0.7708656521638234, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeli Değerlendirme"
      ],
      "metadata": {
        "id": "U2h97Qmg7To5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeli değerlendirme\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "DVHxvhcn04hX",
        "outputId": "3bfbbce3-e833-4837-b671-b9a5d5d3e9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:42]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.36997750401496887, 'eval_runtime': 43.9716, 'eval_samples_per_second': 4.548, 'eval_steps_per_second': 0.569, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Değerlendirme: Modelin performansı test veri seti üzerinde değerlendirilir ve sonuçlar analiz edilir."
      ],
      "metadata": {
        "id": "LP-WPmDGF8co"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tahmin Yapma"
      ],
      "metadata": {
        "id": "Lv9nsrLG7XPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    return predictions.item()\n",
        "\n",
        "# Örnek tahmin yapma\n",
        "sample_text = \"Microsoft announces new AI technologies\"\n",
        "predicted_label = predict(sample_text)\n",
        "print(f\"Predicted label: {predicted_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx-7EKcx079_",
        "outputId": "cdd5ed49-22f8-40f3-d68e-3cb4c18aa15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tahmin yapma fonksiyonu\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    return predictions.item()\n",
        "\n",
        "# Test için örnek metinler\n",
        "sample_texts = [\n",
        "    \"Microsoft announces new AI technologies\",\n",
        "    \"The latest sports news and scores\",\n",
        "    \"Stock market crashes amid economic uncertainty\",\n",
        "    \"New discoveries in quantum physics\"\n",
        "]\n",
        "\n",
        "# Etiketler\n",
        "labels = [\"World\", \"Sports\", \"Business\", \"Science\"]\n",
        "\n",
        "# Tahminleri yapma ve yazdırma\n",
        "print(\"\\nSample Predictions:\")\n",
        "for text in sample_texts:\n",
        "    predicted_label = predict(text)\n",
        "    predicted_label_name = labels[predicted_label]\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Label: {predicted_label_name}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8bb8uP9099J",
        "outputId": "43ff767a-d880-4ccc-a754-e35492c22be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Predictions:\n",
            "Text: Microsoft announces new AI technologies\n",
            "Predicted Label: Science\n",
            "\n",
            "Text: The latest sports news and scores\n",
            "Predicted Label: Science\n",
            "\n",
            "Text: Stock market crashes amid economic uncertainty\n",
            "Predicted Label: Business\n",
            "\n",
            "Text: New discoveries in quantum physics\n",
            "Predicted Label: Science\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kod, AG News veri seti kullanarak metin sınıflandırma işlemini gerçekleştirir. Kodun temel adımları veri yükleme, tokenizasyon, model oluşturma, eğitim, değerlendirme ve tahmin yapmayı içerir. Eğitim süreci boyunca modeli eğitir, değerlendirme sonuçlarını alır ve bazı örnek metinler üzerinde tahminler yaparak modelin performansını gösterir."
      ],
      "metadata": {
        "id": "zp3on7FLGR0_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yjqnDngUUR1a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}